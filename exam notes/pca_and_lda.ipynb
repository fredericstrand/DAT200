{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset into a new coordinate system such that the greatest variance lies along the first axis (the first principal component), the second greatest along the second axis, and so on.\n",
    "\n",
    "Given a dataset $X \\in \\mathbb{R}^{m \\times n}$ with $m$ samples and $n$ features, PCA performs the following steps:\n",
    "\n",
    "1. **Center the data**:\n",
    "   $$\n",
    "   X_{\\text{centered}} = X - \\bar{X}\n",
    "   $$\n",
    "\n",
    "2. **Compute the covariance matrix**:\n",
    "   $$\n",
    "   \\Sigma = \\frac{1}{m} X_{\\text{centered}}^\\top X_{\\text{centered}}\n",
    "   $$\n",
    "\n",
    "3. **Compute the eigenvalues and eigenvectors** of $\\Sigma$:\n",
    "   $$\n",
    "   \\Sigma \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\n",
    "   $$\n",
    "\n",
    "4. **Select the top $k$ eigenvectors** (principal components) corresponding to the largest eigenvalues and form the projection matrix:\n",
    "   $$\n",
    "   W_k = [\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k]\n",
    "   $$\n",
    "\n",
    "5. **Project the data** onto the new $k$-dimensional subspace:\n",
    "   $$\n",
    "   Z = X_{\\text{centered}} W_k\n",
    "   $$\n",
    "\n",
    "PCA reduces noise and redundancy in the data while preserving the directions of maximum variance. It is widely used for data compression, visualization, and preprocessing before machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "        cov = np.cov(X_centered, rowvar=False)\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return np.dot(X_centered, self.components)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique that finds a linear combination of features that best separates two or more classes. Unlike PCA, LDA uses class label information to maximize class separability.\n",
    "\n",
    "Given a dataset with $C$ classes, LDA computes two scatter matrices:\n",
    "\n",
    "- **Within-class scatter**:\n",
    "  $$\n",
    "  S_W = \\sum_{c=1}^{C} \\sum_{\\mathbf{x}_i \\in \\mathcal{D}_c} (\\mathbf{x}_i - \\boldsymbol{\\mu}_c)(\\mathbf{x}_i - \\boldsymbol{\\mu}_c)^\\top\n",
    "  $$\n",
    "\n",
    "- **Between-class scatter**:\n",
    "  $$\n",
    "  S_B = \\sum_{c=1}^{C} n_c (\\boldsymbol{\\mu}_c - \\boldsymbol{\\mu})(\\boldsymbol{\\mu}_c - \\boldsymbol{\\mu})^\\top\n",
    "  $$\n",
    "\n",
    "where \\( \\boldsymbol{\\mu}_c \\) is the mean of class \\( c \\), \\( \\boldsymbol{\\mu} \\) is the overall mean, and \\( n_c \\) is the number of samples in class \\( c \\).\n",
    "\n",
    "LDA finds the projection matrix \\( W \\) that maximizes the ratio:\n",
    "\n",
    "$$\n",
    "W = \\arg\\max_W \\frac{|W^\\top S_B W|}{|W^\\top S_W W|}\n",
    "$$\n",
    "\n",
    "This leads to solving the generalized eigenvalue problem:\n",
    "\n",
    "$$\n",
    "S_W^{-1} S_B \\mathbf{w} = \\lambda \\mathbf{w}\n",
    "$$\n",
    "\n",
    "The top $k$ eigenvectors form the transformation matrix, and the data is projected onto this subspace for classification or visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.linear_discriminants = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        class_labels = np.unique(y)\n",
    "        mean_overall = np.mean(X, axis=0)\n",
    "        S_W = np.zeros((n_features, n_features))\n",
    "        S_B = np.zeros((n_features, n_features))\n",
    "\n",
    "        for c in class_labels:\n",
    "            X_c = X[y == c]\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            S_W += np.dot((X_c - mean_c).T, (X_c - mean_c))\n",
    "            n_c = X_c.shape[0]\n",
    "            mean_diff = (mean_c - mean_overall).reshape(n_features, 1)\n",
    "            S_B += n_c * np.dot(mean_diff, mean_diff.T)\n",
    "\n",
    "        A = np.linalg.inv(S_W).dot(S_B)\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "        idx = np.argsort(np.abs(eigenvalues))[::-1]\n",
    "        self.linear_discriminants = eigenvectors[:, idx[:self.n_components]].real\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.dot(X, self.linear_discriminants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
