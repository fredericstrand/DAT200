{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first ML algorithms emerging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "The perceptron is a binary linear classifier that maps an input vector $ \\mathbf{x} \\in \\mathbb{R}^n $ to an output $ y \\in \\{0, 1\\} $ using a weight vector $ \\mathbf{w} \\in \\mathbb{R}^n $ and bias $ b \\in \\mathbb{R} $. The decision function is:\n",
    "\n",
    "$$\n",
    "y = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\mathbf{w}^\\top \\mathbf{x} + b \\geq 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Learning updates the weights via:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta(y - \\hat{y})\\mathbf{x}\n",
    "$$\n",
    "\n",
    "where $ \\eta $ is the learning rate and $ \\hat{y} $ is the predicted label.\n",
    "\n",
    "\n",
    "![Perceptron](images/perceptron_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, n_features, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear = np.dot(X, self.weights) + self.bias\n",
    "        return np.where(linear >= 0, 1, 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_iters):\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.lr * (target - self.predict(xi))\n",
    "                self.weights += update * xi\n",
    "                self.bias += update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaline\n",
    "\n",
    "The Adaline (Adaptive Linear Neuron) is a binary classifier that uses a linear activation function. Given input vector $ \\mathbf{x} \\in \\mathbb{R}^n $, weights $ \\mathbf{w} \\in \\mathbb{R}^n $, and bias $ b \\in \\mathbb{R} $, the net input is:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "The prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } z \\geq 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The weights are updated using the rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\sum_i (y_i - \\hat{y}_i) \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "b \\leftarrow b + \\eta \\sum_i (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "where $ \\eta $ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline:\n",
    "    def __init__(self, n_features, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0, 1, 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_iters):\n",
    "            output = self.net_input(X)\n",
    "            errors = y - output\n",
    "            self.weights += self.lr * np.dot(X.T, errors)\n",
    "            self.bias += self.lr * errors.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron vs Adaline\n",
    "\n",
    "### Similarities:\n",
    "- Both are classifiers for binary classification\n",
    "- Both have linear decision boundary\n",
    "- Both use a threshold function\n",
    "\n",
    "### Differences:\n",
    "\n",
    "- **Perceptron** uses a step function $ \\phi(z) $ as its activation. It compares predicted class labels to the true labels and updates weights immediately after each misclassification (online learning). This can result in frequent updates within an epoch.\n",
    "\n",
    "- **Adaline** uses a linear activation function $ \\phi(z) = z$), which is simply the net input. It compares the continuous output to the true labels and performs batch updates at the end of each epoch, based on the total error across all training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "**Linear Regression** models the relationship between an input vector $ \\mathbf{x} \\in \\mathbb{R}^n $ and a continuous target variable $ y \\in \\mathbb{R} $. It assumes the output is a linear combination of the inputs:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "The model is trained by minimizing the **mean squared error** (MSE):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "$$\n",
    "\n",
    "where $ m $ is the number of training samples. The weights $ \\mathbf{w} $ and bias $ b $ are updated using gradient descent to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            dw = -(2/n_samples) * np.dot(X.T, (y - y_pred))\n",
    "            db = -(2/n_samples) * np.sum(y - y_pred)\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) are supervised learning models used for classification and regression. In binary classification, an SVM aims to find the optimal hyperplane that maximally separates two classes in the feature space.\n",
    "\n",
    "Given training data $ \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^m $, where $ \\mathbf{x}_i \\in \\mathbb{R}^n $ and $ y_i \\in \\{-1, +1\\} $, the SVM solves the optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b} \\ \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "$$\n",
    "y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for all } i.\n",
    "$$\n",
    "\n",
    "This is known as the hard-margin SVM, suitable for linearly separable data.\n",
    "\n",
    "For non-separable data, a soft-margin SVM introduces slack variables \\( \\xi_i \\geq 0 \\) and a penalty parameter \\( C > 0 \\):\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\ \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^m \\xi_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\quad \\text{and} \\quad \\xi_i \\geq 0.\n",
    "$$\n",
    "\n",
    "The decision function for prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)\n",
    "$$\n",
    "\n",
    "SVMs can be extended to non-linear classification using the kernel trick, which maps input data into a higher-dimensional space:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "Common kernels include the polynomial kernel and the radial basis function (RBF) kernel. The key idea is that the SVM algorithm depends only on inner products, so we don't need to compute $ \\phi(\\mathbf{x}) $ explicitly.\n",
    "\n",
    "SVMs are powerful because they maximize the margin between classes and are robust to overfitting, especially in high-dimensional spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, lr=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.weights) + self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.lr * (2 * self.lambda_param * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.lr * (2 * self.lambda_param * self.weights - np.dot(x_i, y_[idx]))\n",
    "                    self.bias -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.weights) + self.bias\n",
    "        return np.sign(approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning algorithm used for classification and regression. Given a query point $ \\mathbf{x} \\in \\mathbb{R}^n $, KNN identifies the $k$ closest training samples in the feature space using a distance metric, commonly the Euclidean distance:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{x}, \\mathbf{x}_i) = \\| \\mathbf{x} - \\mathbf{x}_i \\|_2\n",
    "$$\n",
    "\n",
    "For classification, the algorithm assigns the most frequent label among the $k$ nearest neighbors:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{mode} \\left( \\{ y_i \\ | \\ \\mathbf{x}_i \\in \\mathcal{N}_k(\\mathbf{x}) \\} \\right)\n",
    "$$\n",
    "\n",
    "For regression, it typically takes the average of the neighborsâ€™ target values:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{k} \\sum_{\\mathbf{x}_i \\in \\mathcal{N}_k(\\mathbf{x})} y_i\n",
    "$$\n",
    "\n",
    "KNN is simple, interpretable, and effective for low-dimensional data, but becomes computationally expensive and less accurate in high-dimensional spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(x) for x in X])\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_labels).most_common(1)\n",
    "        return most_common[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
